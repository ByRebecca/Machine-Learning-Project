# -*- coding: utf-8 -*-


Automatically generated by Colab.


**Machine Learning End Project**

FeedForward Neural Network


(Master 2 APE)




*   Rebecca CLIPAL
*   Sarah JILAL
"""

import matplotlib as mpl
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
import random
import math
from keras import layers
from keras import models
from keras import optimizers
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import PolynomialFeatures
import copy
import operator

"""# Import the dataset

This dataset includes 303 entries and 15 columns, containing various medical attributes such as age, sex, cholesterol levels, resting blood pressure, and more. The features also include types of chest pain, fasting blood sugar levels, resting electrocardiographic results, maximum heart rate achieved during a stress test, and others such as exercise-induced angina, ST depression induced by exercise, the slope of the peak exercise ST segment, number of major vessels colored by fluoroscopy, thalassemia, and a diagnosis of heart disease ('AHD'). The dataset appears to be a mix of numerical and categorical variables and is likely used for analyzing factors that contribute to heart disease.
"""

heart = pd.read_csv('/content/sample_data/Heart.csv')
print(heart)

heart.describe()
heart["AHD"].value_counts()
heart['AHD'] = heart['AHD'] == 'Yes'
heart.head()

"""*Variable of interest: AHD (binary variable, adult heart disease). No missing variables.*

#Creation of test and train sets.

We first calculate the number of test samples (`n_test`) by taking 30% of the length of the `heart` dataset, rounding up to the nearest whole number. We set the random seed to 42 to ensure reproducibility of our results. Then, we randomly select indices for our test set (`test_ixs`) from the range of indices in the `heart` dataset. The indices not selected for the test set are used for the training set (`train_ixs`). Using these indices, we create two subsets from the `heart` dataset: `heart_train` for training and `heart_test` for testing. Finally, we print the lengths of the training and testing datasets to verify their sizes.
"""

n_test = int(math.ceil(len(heart) * 0.3))
random.seed(42)
test_ixs = random.sample(list(range(len(heart))), n_test)
train_ixs = [ix for ix in range(len(heart)) if ix not in test_ixs]
heart_train = heart.iloc[train_ixs, :]
heart_test = heart.iloc[test_ixs, :]
print(len(heart_train))
print(len(heart_test))

"""We continue by focusing on a specific explanatory variable, `AHD`, and an outcome variable, `chol`, which is numeric with a minimum of 94, an average of 247, and a maximum of 564. We partition the `heart` dataset into training and testing subsets for both variables: `x_train` and `x_test` for cholesterol levels (`Chol`), and `y_train` and `y_test` for the presence of arterial heart disease (`AHD`)."""

x_train = heart_train['Chol']
y_train = heart_train['AHD']

x_test = heart_test['Chol']
y_test = heart_test['AHD']

x_train.head()
y_train.head()

"""We reshape the training and testing sets for both the explanatory variable (`chol`) and the outcome variable (`AHD`) into column vectors. This step is necessary to ensure that the data is in the correct format for most machine learning algorithms, which expect input arrays to be two-dimensional.

Normalization can significantly speed up the training process of algorithms, particularly those that are sensitive to the scale of input features, such as gradient descent-based algorithms. By ensuring all features contribute equally, the algorithm can converge to the optimum solution more efficiently.

Without normalization, features on larger scales can dominate the training process, while features on smaller scales may not be considered properly by the model. Normalizing ensures that each feature has the opportunity to influence the model equally, based on its relevance, rather than its scale.

For algorithms that rely on calculating distances between data points, such as k-Nearest Neighbors (k-NN) and k-Means clustering, normalization is crucial. It ensures that the distance measure accurately reflects the similarities or differences between data points, without being skewed by the scale of different features.
"""

x_train = x_train.values.reshape(-1,1)
y_train = y_train.values.reshape(-1,1)

x_test = x_test.values.reshape(-1,1)
y_test = y_test.values.reshape(-1,1)

"""# First simple neural network model.

In this code, we define a neural network model using TensorFlow Keras with a specific structure aimed at a particular task. Our model, named 'My_two_neurons_model_fixedWeights', comprises one hidden layer with 2 neurons and one output layer with a single neuron.

- **Hidden Layer**: This layer uses the 'relu' (Rectified Linear Unit) activation function, which outputs the maximum between 0 and the input value. This choice of activation helps introduce non-linearity into the model, enabling it to learn more complex patterns. The `input_shape=(1,)` specifies that our model expects input with a single feature.

- **Output Layer**: The output layer contains one neuron and uses a 'linear' activation function. This setup is typical for regression tasks, where the goal is to predict a continuous value. The linear activation function means the output is a direct weighted sum of the inputs plus a bias, allowing the model to make predictions within a range that is not bounded (as opposed to classifications tasks where the output is often bounded between 0 and 1).

This structure is concise yet capable of modeling complex relationships by adjusting the weights through training. The choice of activation functions ('relu' for hidden layers and 'linear' for the output layer) is standard for many regression and basic classification tasks, balancing model complexity and computational efficiency.
"""

model = models.Sequential(name='My_two_neurons_model_fixedWeights')

# hidden layer with 2 neurons (or nodes)
model.add(layers.Dense(2, activation='relu', input_shape=(1,)))

# output layer, one neuron
model.add(layers.Dense(1,  activation='linear'))

model.summary()

"""
- Optimizer is a method used to minimize the loss function by iteratively updating the model parameters (weights and biases). The learning rate of `0.02` is the size of the steps that the optimizer takes towards the minimum of the loss function. A learning rate that's too high might overshoot the minimum, while a learning rate that's too low might take too long to converge or get stuck in a local minimum.

- Loss Function is a common loss function for regression tasks. It calculates the average of the squares of the errors between the predictions made by the model and the actual values. The model's objective during training is to minimize this value, which corresponds to finding the best set of parameters that makes the predictions as close as possible to the true outcomes.

By compiling the model with these settings, we prepare it to be trained on our dataset. The optimizer will adjust the weights based on the computed gradients of the loss function, and over multiple iterations (epochs), the model should ideally learn to reduce the loss and improve its predictions."""

sgd = optimizers.SGD(learning_rate=0.02)
model.compile(loss='mse',optimizer=sgd)

"""## Train this model.

We train our model using the `fit` method, passing our training data `x_train` and `y_train` as inputs. We set it to run for 100 epochs, meaning the model will work through the entire training set 100 times. We use a batch size of 16, so the model updates weights after every 16 samples processed. This training process is designed to optimize the model's performance by iteratively reducing the loss across the epochs.
"""

history = model.fit(x_train, y_train, epochs=100, batch_size=16)

"""We assess our model's performance by evaluating it on both the training and testing datasets using a batch size of 32. The `evaluate` function returns the loss value for the given data, which we refer to as accuracy in this context (though typically, 'accuracy' refers to a classification metric). We print out the calculated 'accuracy' for both the training and testing sets to compare.

Next, we extract the loss values recorded during training from the `history` object and plot them using Matplotlib. The loss values are plotted against the number of epochs, represented on the x-axis, with the color red. This graph helps us visually assess how the loss decreases over time, indicating how well the model is learning from the training data.

The training and testing loss values are approximately 0.2489 and 0.2471, respectively. These values are very close, indicating that the model is performing similarly on both the training and testing sets. There is no significant sign of overfitting, as the testing loss is not substantially higher than the training loss.
"""

train_acc = model.evaluate(x_train, y_train, batch_size=32)
test_acc = model.evaluate(x_test, y_test, batch_size=32)
print('Training accuracy: %s' % train_acc)
print('Testing accuracy: %s' % test_acc)

losses = history.history['loss']
plt.plot(range(len(losses)), losses, 'r')
plt.show()

"""Same process but with a different code

"""

plt.plot(history.history['loss'],'b')
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')

weights_t = model.get_weights()

print("TF weights:\n", weights_t)

"""# Second model : more neurons and we train it more times

In this model, we have defined a sequential architecture with the following layers:

- **dense_16 (Dense)**: A dense layer with 20 neurons, using the 'relu' activation function. The output shape is (None, 20), indicating that the layer will output 20 features for each input sample. This layer has 40 parameters; since it's the first layer, it has 20 weights (one for each neuron) and 20 biases (one for each neuron).

- **dense_17 (Dense)**: The output layer with a single neuron, using a 'linear' activation function, suitable for regression tasks. The output shape is (None, 1), which means it will output a single continuous value. This layer has 21 parameters, accounting for 20 weights from the connections to the previous layer's 20 neurons and 1 bias.

The model has a total of 61 trainable parameters, which will be adjusted during the training process to minimize the loss function. There are no non-trainable parameters, suggesting that all parameters will be updated through training.

This model is more complex than the previously discussed model with only 2 neurons in the hidden layer, as it has a larger number of parameters due to the increased number of neurons in the hidden layer. This increased complexity allows the model to potentially learn more nuanced patterns in the data.
"""

model2 = models.Sequential(name='My_two_neurons_model_fixedWeights')
model2.add(layers.Dense(20, activation='relu', input_shape=(1,)))
model2.add(layers.Dense(1,  activation='linear'))

model2.summary()

sgd = optimizers.SGD(learning_rate=0.02)
model2.compile(loss='mse',optimizer=sgd)

history2 = model2.fit(x_train, y_train, epochs=1000, batch_size=16)

"""**Results** :

- The training loss is approximately 0.2489, and the testing loss is about 0.2471. These values are quite close, suggesting that our model is performing consistently across both datasets without significant overfitting or underfitting.

- The plotted graph shows the loss over 1000 epochs, as indicated by the x-axis which extends from 0 to 1000. The loss values fluctuate considerably throughout the training process, as represented by the red spikes in the graph. However, despite these fluctuations, the overall loss remains within a narrow range, indicating a stability in the model's learning process.

In short, we have trained a stable model that performs similarly on both seen (training) and unseen (testing) data, as evidenced by the loss values.
"""

train_acc = model2.evaluate(x_train, y_train, batch_size=32)
test_acc = model2.evaluate(x_test, y_test, batch_size=32)
print('Training accuracy: %s' % train_acc)
print('Testing accuracy: %s' % test_acc)

losses = history2.history['loss']
plt.plot(range(len(losses)), losses, 'r')
plt.show()

"""We observe more or less the same results, indicating no improvement in the model's performance. Hence, we should consider testing other output activation functions. Since our explained variable is binary, it would be more appropriate to use a sigmoid function for the output layer.

# Third model : with the sigmoïd activaton function (output layer)

## Basic Model

We've developed a third model incorporating a sigmoid activation function in the output layer, which is typically used for binary classification tasks.

We've created `model3` using the Keras Sequential API, named "My_two_neurons_model_fixedWeights", similar to our previous models. This model consists of a hidden layer with 20 neurons (using 'relu' activation) and an output layer with one neuron (using 'sigmoid' activation). The 'relu' activation function will allow the hidden layer to capture non-linear patterns, and the 'sigmoid' in the output layer will output a probability ranging between 0 and 1, indicating the likelihood of the input being classified as class 1.

The summary shows that `model3` has 61 trainable parameters, with 40 parameters in the hidden layer and 21 in the output layer. There are no non-trainable parameters, meaning all 61 parameters will be optimized during training.

We've compiled the model using the Mean Squared Error (MSE) loss function and Stochastic Gradient Descent (SGD) as the optimizer with a learning rate of 0.02. While MSE is not typical for binary classification tasks, it might still be used experimentally. However, for binary outcomes, binary cross-entropy is usually preferred.

Finally, the model is trained with `model3.fit`, specifying our training data (`x_train`, `y_train`), the number of epochs (1000), and the batch size (16). Training for 1000 epochs indicates we're allowing the model ample opportunity to learn from the data, with the potential risk of overfitting, which should be monitored.
"""

model3 = models.Sequential(name='My_two_neurons_model_fixedWeights')
model3.add(layers.Dense(20, activation='relu', input_shape=(1,)))
model3.add(layers.Dense(1,  activation='sigmoid'))

model3.summary()

sgd = optimizers.SGD(learning_rate=0.02)
model3.compile(loss='mse',optimizer=sgd)

history3 = model3.fit(x_train, y_train, epochs=1000, batch_size=16)

"""We see that our model `model3` has achieved a training loss (referred to here as accuracy) of approximately 0.5330 and a testing loss of about 0.5604. The graph indicates that the loss has plateaued and does not improve significantly as the number of epochs increases. This suggests that while the model may be slightly better, it is not learning further from the data over time. We might need to explore additional changes to our model or data to achieve further improvements."""

train_acc = model3.evaluate(x_train, y_train, batch_size=32)
test_acc = model3.evaluate(x_test, y_test, batch_size=32)
print('Training accuracy: %s' % train_acc)
print('Testing accuracy: %s' % test_acc)

losses = history3.history['loss']
plt.plot(range(len(losses)), losses, 'r')
plt.show()

"""## Adding layers and other optimizer and loss function

We've constructed a new neural network model, `model4`, which is an extension of our previous efforts. This model consists of multiple dense layers with 'relu' activations for the hidden layers and a 'sigmoid' activation for the output layer, suitable for binary classification.

- The first layer, `dense_10`, has 20 neurons.
- The second layer, `dense_11`, and the third layer, `dense_12`, both have 10 neurons each.
- The final layer, `dense_13`, is the output layer with 1 neuron using the 'sigmoid' function to predict a binary outcome.

The summary indicates that the model has a total of 371 parameters, which likely include weights and biases across all layers. The layers are stacked in a way that the output of one layer is the input to the next, with 'relu' providing non-linear activation functions for the hidden layers, allowing the model to learn complex patterns. The 'sigmoid' in the output layer is designed to output a probability between 0 and 1.

With more layers and neurons compared to previous models, `model4` has increased complexity and capacity to model more intricate relationships in the data.
"""

model4 = models.Sequential(name='My_two_neurons_model_fixedWeights')
model4.add(layers.Dense(20, activation='relu', input_shape=(1,)))
model4.add(layers.Dense(10, activation='relu', input_shape=(1,)))
model4.add(layers.Dense(10, activation='relu', input_shape=(1,)))
model4.add(layers.Dense(1,  activation='sigmoid'))

model4.summary()

"""### Loss function: MSE"""

model4.compile(loss='mse',optimizer='adam')
history4 = model4.fit(x_train, y_train, epochs=1000, batch_size=16)

"""We have trained `model4` and evaluated its performance on both the training and testing datasets. Our training loss is about 0.2489, and our testing loss is roughly 0.2471, which are quite close to each other, indicating that we have managed to maintain a good balance and avoid overfitting.

Looking at the loss plot over 1000 epochs, we see considerable fluctuations, but the overall loss remains within a narrow range. This suggests that while our model is learning, it might have reached its capacity in terms of learning from the data provided, given the current architecture and hyperparameters.
"""

train_acc = model4.evaluate(x_train, y_train, batch_size=32)
test_acc = model4.evaluate(x_test, y_test, batch_size=32)
print('Training accuracy: %s' % train_acc)
print('Testing accuracy: %s' % test_acc)

losses = history4.history['loss']
plt.plot(range(len(losses)), losses, 'r')
plt.show()

"""### New loss function: Binary Crossentropy Loss

We compile `model4` by setting 'binary_crossentropy' as the loss function, which is appropriate for binary classification problems. We choose 'adam' as our optimizer, a popular choice that adapts the learning rate during training. We also track 'accuracy' as a metric to evaluate our model's performance.

Next, we fit `model4` to our training data `x_train` and `y_train` over 1000 epochs with a batch size of 16, indicating the number of samples per gradient update. We include `x_test` and `y_test` as our validation data, which the model will evaluate after each epoch to provide insight into how well it generalizes to unseen data. This extended training and the use of validation data help us monitor and mitigate overfitting while aiming to improve the model's predictive accuracy.
"""

model4.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])
history4 = model4.fit(x_train, y_train, epochs=1000, batch_size=16, validation_data=(x_test, y_test))

"""We have evaluated our model model4 on the training and test datasets. The evaluation returned two values: loss and accuracy. For training, we obtained a loss of around 0.6910 with an accuracy of approximately 0.5330. On the test dataset, the loss was about 0.6873 with an accuracy of nearly 0.5604. These results suggest that our model's ability to predict accurately is around 53-56%, which may not be highly effective for a binary classification task.

In the loss plot over 1000 epochs, we notice that the loss remains relatively stable with minor fluctuations throughout the training process. The plot shows no clear trend towards improvement, indicating that the model is not significantly improving with more epochs. The consistent loss values close to 0.69 suggest that the model may be guessing rather than learning from the data, as this value is close to the loss expected from a model predicting outcomes at random for a balanced binary classification problem.

Based on the loss values, earlier models seemed to perform better. However, when considering accuracy which is a more intuitive measure of performance in classification tasks only the latest model (model4) provided this metric, reporting a modest 53-56% accuracy on the training and test sets.

"""

train_acc = model4.evaluate(x_train, y_train, batch_size=32)
test_acc = model4.evaluate(x_test, y_test, batch_size=32)
print('Training accuracy: %s' % train_acc)
print('Testing accuracy: %s' % test_acc)

losses = history4.history['loss']
plt.plot(range(len(losses)), losses, 'r')
plt.show()

"""# Reviewing results

"""

y_pred = model4.predict(x_test)
plt.scatter(x_test, y_test, color='blue', label='True Values')
plt.scatter(x_test, y_pred, color='red', label='Predictions')
plt.xlabel('Chol')
plt.ylabel('AHD')
plt.title('True Values vs Predictions')
plt.legend()
plt.show()

"""# Under or overfitting ?

We have maintained the structure from our previous model, including three hidden layers with ReLU activation functions and one output layer with a sigmoid activation. We continue to use binary crossentropy as our loss function and ADAM as our optimizer. Now, we aim to compare the learning curves on both the training and test sets to assess how our model performs and generalizes during the training process. This comparison will help us understand the model's behavior and ensure that it is learning effectively from the data without overfitting.
"""

model4 = models.Sequential(name='My_two_neurons_model_fixedWeights')
model4.add(layers.Dense(20, activation='relu', input_shape=(1,)))
model4.add(layers.Dense(10, activation='relu', input_shape=(1,)))
model4.add(layers.Dense(10, activation='relu', input_shape=(1,)))
model4.add(layers.Dense(1,  activation='sigmoid'))

model4.compile(loss='binary_crossentropy',optimizer='adam', metrics="accuracy")

history4 = model4.fit(x_train, y_train, epochs=1000, batch_size=16,validation_data=(x_test, y_test))

"""We have plotted the accuracy of our model over 1000 epochs for both the training set and the validation (test) set. In the graph, we aimed to visually compare the 'Train Accuracy' and 'Test Accuracy' to evaluate how well our model is learning and generalizing.

The graph shows us that the accuracy on both the training and validation sets plateaus as the number of epochs increases. This stagnation in accuracy suggests that our model might not be able to learn any further from the data, a situation that could indicate underfitting. Underfitting occurs when the model is too simple to capture the underlying pattern in the data. In response to this, we might consider increasing the model complexity or adjusting the features we're using for training to improve our model's learning capability.
"""

plt.plot(history4.history['accuracy'], label='Train Accuracy')
plt.plot(history4.history['val_accuracy'], label='Test Accuracy')
plt.title('Train and Test Accuracy Over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

"""# Correction of underfitting

## Train less

In an attempt to correct underfitting, we adjust our training strategy. We decide to train our model for fewer epochs—300 instead of 1000. The reasoning behind this could be to observe whether a shorter training period might prevent the model from stagnating early on and potentially improve its ability to learn. We retain the batch size of 16, which dictates how many samples are processed before the model's internal parameters are updated. Validation data is still used to monitor the model's performance on the test set. This change is an experiment to see if training for fewer epochs helps the model generalize better without compromising on learning the underlying patterns in the data.
"""

history4_corr_1 = model4.fit(x_train, y_train, epochs=300, batch_size=16,validation_data=(x_test, y_test))

plt.plot(history4_corr_1.history['accuracy'], label='Train Accuracy')
plt.plot(history4_corr_1.history['val_accuracy'], label='Test Accuracy')
plt.title('Train and Validation Accuracy Over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

"""The graph does not show an improvement in the model's performance as a result of the changes made to address underfitting. This indicates that simply reducing the number of epochs may not be sufficient to enhance the model's learning capability and that further adjustments may be necessary.

## Correction od the underfitting with a *Regulizer*

We've initiated a new approach to refine our model, `model4_corr_2`, by incorporating regularization techniques. This model is similar in structure to our previous models with three hidden layers and a sigmoid output layer. However, the key difference is the addition of L2 regularization to one of the hidden layers. The intent behind using L2 regularization is to improve the model's ability to generalize by discouraging complexity in the model's learned parameters. By adding this regularization, we aim to achieve a better balance between the model's complexity and its performance on unseen data.
"""

kernel_weight = 1
bias_weight = 1



model4_corr_2 = models.Sequential(name='Model_with_Regulizer')
model4_corr_2.add(layers.Dense(20, activation='relu', input_shape=(1,)))
model4_corr_2.add(layers.Dense(10, activation='relu', input_shape=(1,)))
model4_corr_2.add(layers.Dense(10, activation='relu',
                kernel_regularizer=tf.keras.regularizers.l2(kernel_weight),
                bias_regularizer=tf.keras.regularizers.l2(bias_weight)))
model4_corr_2.add(layers.Dense(1,  activation='sigmoid'))



model4_corr_2.compile(loss='binary_crossentropy',optimizer='adam', metrics="accuracy")

history4_corr_2 = model4_corr_2.fit(x_train, y_train, epochs=1000, batch_size=16,validation_data=(x_test, y_test))

"""
Training accuracy is around 69%, and testing accuracy is approximately 56%. Thre are significant fluctuations but they seem to hover around a specific mean value. The graph and the reported accuracies indicate that while the model is achieving a moderate level of accuracy, it has not significantly improved despite the regularization changes. The consistency in loss values suggests that the model may have reached its learning capacity given the current architecture and hyperparameters. The fluctuations could also be due to the high regularization penalties applied, which might be too strong and hindering the model's ability to fit the training data."""

train_acc = model4_corr.evaluate(x_train, y_train, batch_size=32)
test_acc = model4_corr.evaluate(x_test, y_test, batch_size=32)
print('Training accuracy: %s' % train_acc)
print('Testing accuracy: %s' % test_acc)

losses = history4_corr_2.history['loss']
plt.plot(range(len(losses)), losses, 'r')
plt.show()

"""We use our model to make predictions on the test dataset and plot these predictions against the true values. This graphical representation allows us to quickly assess how well our model's predictions align with the actual data, specifically looking at the relationship between cholesterol levels ('Chol') and the presence of arterial heart disease ('AHD').





"""

y_pred = model4_corr.predict(x_test)
plt.scatter(x_test, y_test, color='blue', label='True Values')
plt.scatter(x_test, y_pred, color='red', label='Predictions')
plt.xlabel('Chol')
plt.ylabel('AHD')
plt.title('True Values vs Predictions')
plt.legend()
plt.show()

plt.plot(history4_corr_2.history['accuracy'], label='Train Accuracy')
plt.plot(history4_corr_2.history['val_accuracy'], label='Test Accuracy')
plt.title('Train and Test Accuracy Over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

"""Unfortunately, we still find underfitting

# More explanatory variables

Finally, we tried to add multiple explanatory variables to increasing our model's accuracy.
"""

from sklearn import preprocessing

"""We select three features: 'Chol' (cholesterol levels), 'Age', and 'RestECG' (resting electrocardiographic results)

We then normalize these features in both the training set (`x_train_new`) and the test set (`x_test_new`).

After normalization, we print the `x_train_new` dataset, which shows the normalized values of 'Chol', 'Age', and 'RestECG' for 212 observations. The output indicates the structure of the `x_train_new` dataframe with 212 rows and 3 columns, corresponding to the selected features.
"""

x_train_new = heart_train[['Chol', 'Age', 'RestECG']]
x_test_new = heart_test[['Chol', 'Age', 'RestECG']]

preprocessing.normalize(x_train_new)
preprocessing.normalize(x_test_new)

print(x_train_new)

"""We've constructe a neural network named "Multiple_variables" with four layers using Keras. The first layer has 20 neurons, and we specify our input data will have three features. Two subsequent layers have 10 neurons each, and the final layer has a single neuron with a sigmoid activation function for binary classification. The model summary indicates that there are 411 trainable parameters in total."""

model5 = models.Sequential(name='Multiple_variables')
model5.add(layers.Dense(20, activation='relu', input_shape=(3,)))
model5.add(layers.Dense(10, activation='relu', input_shape=(3,)))
model5.add(layers.Dense(10, activation='relu', input_shape=(3,)))
model5.add(layers.Dense(1,  activation='sigmoid'))

model5.summary()

model5.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])
history5_pls_var = model5.fit(x_train_new, y_train, epochs=1000, batch_size=16)

"""We evaluate our neural network's performance, achieving around 62.3% accuracy on our training data and approximately 62.6% on our test data.  We were able to slightly increase our training and testing accuracy. Our model learns over the epochs, then reach a plateau after, around, 800 epochs."""

train_acc = model5.evaluate(x_train_new, y_train, batch_size=32)
test_acc = model5.evaluate(x_test_new, y_test, batch_size=32)
print('Training accuracy: %s' % train_acc)
print('Testing accuracy: %s' % test_acc)

losses = history5_pls_var.history['loss']
plt.plot(range(len(losses)), losses, 'r')
plt.show()
